{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression to predict wine score\n",
        "\n",
        "## Task1. Train a linear model\n",
        "    - using least squares method\n",
        "    - implement the model from scratch using NumPy \n",
        "    - Use learning curves plot to understand whether the linear moel is overfitting or underfitting\n",
        "We will use the winequality dataset for this practical. The original dataset is available here:\n",
        "https://archive.ics.uci.edu/ml/datasets/Wine+Quality. \n",
        "In order to make it easier to import the dataset, the dataset has been converted to the numpy array format and shuffled. you can find out the datset in dataset repository.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3_7wEbf960iI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 load the package"
      ],
      "metadata": {
        "id": "x8GxUaec74Ai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import _pickle as cp"
      ],
      "metadata": {
        "id": "lIYWKGTV7yW0"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 load the dataset "
      ],
      "metadata": {
        "id": "M31Dzahi7x4W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the white wine dataset\n",
        "# X is the feature matrix that stores the feature values of the data records\n",
        "# y is the label vector that stores the labels of the data records\n",
        "X, y = cp.load(open('winequality-white.pickle', 'rb'))\n",
        "\n",
        "# check the size of the data\n",
        "print(\"X is a {} matrix, which contains {} data records and {} features.\".format(X.shape, X.shape[0], X.shape[1]))\n",
        "print(\"y is a {}-dimentional vector, which stores the corresponding labels of the data records in X\".format(y.shape[0]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3CvOGr08BRx",
        "outputId": "cdc9ef7c-ed67-4568-eb11-87e3ba6c5227"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X is a (4898, 11) matrix, which contains 4898 data records and 11 features.\n",
            "y is a 4898-dimentional vector, which stores the corresponding labels of the data records in X\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 split traing/test dataset \n",
        "In practice, we should sample randomly 80% of the data as training data and the rest as the test data."
      ],
      "metadata": {
        "id": "hUWNkm1G8V7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def split_data(X, y, split_coeff):\n",
        "  '''\n",
        "  The parameter split_coeff is a percentage value such that\n",
        "  the first split_coeff of the dataset goes to the training dataset \n",
        "  and the remaining data goes to the test dataset.\n",
        "  '''\n",
        "  N, _ = X.shape # get the number of records (rows)\n",
        "  train_size = int(split_coeff * N) # use the first split_coeff of the data as the training data\n",
        "  X_train = X[:train_size] # the first training_size records\n",
        "  y_train = y[:train_size]\n",
        "  X_test = X[train_size:] # the last test_size records\n",
        "  y_test = y[train_size:]\n",
        "  \n",
        "  return X_train, y_train, X_test, y_test\n",
        "\n",
        "X_train, y_train, X_test, y_test = split_data(X, y, 0.8) # split the data with split_coeff=0.8\n",
        "\n",
        "# check the size of the splitted dataset\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "print(\"Shape of X_test:\", X_test.shape)\n",
        "print(\"Shape of y_test:\", y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Cb2mLTb8nM2",
        "outputId": "5fb82552-0602-4bd5-db40-25ce1cc342d9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_train: (3918, 11)\n",
            "Shape of y_train: (3918,)\n",
            "Shape of X_test: (980, 11)\n",
            "Shape of y_test: (980,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4 understand datasets \n",
        "Letâ€™s first check\n",
        "the distribution of the y-values in the training data.  \n",
        "Make a bar chart showing the distribution of y-values in the training data then you will find that the values are integers between 3 and 9 indicating the quality of the wine.  \n"
      ],
      "metadata": {
        "id": "LhvPAsJO7xpz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_bar_chart_score(y_train):\n",
        "\n",
        "    y_train_ints = np.array(y_train)\n",
        "    y_train_counts = np.bincount(y_train_ints.astype('int'))\n",
        "    x_labels = np.nonzero(y_train_counts)[0]\n",
        "\n",
        "    plt.title(\"Distribution of scores of wines\")\n",
        "    plt.xlabel(\"Score\")\n",
        "    plt.ylabel(\"Number of wines\")\n",
        "    plt.bar(x_labels, y_train_counts[y_train_counts != 0])\n",
        "    plt.show()\n",
        "\n",
        "plot_bar_chart_score(y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "8iZrHcUj-GTS",
        "outputId": "1011bc16-41e6-40ac-d20e-22f6591c3cab"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.5 implement baseline model\n",
        "build a trivial predictor, which always returns the mean of the y-values of the training data. The linear regression models we build later should perform better than this trivial model."
      ],
      "metadata": {
        "id": "ucNH45c7_HEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_average(y_train):\n",
        "    return np.mean(y_train)\n",
        "\n",
        "y_train_avg = compute_average(y_train)\n",
        "print(\"Average of y on the training label values is {}\".format(y_train_avg))\n",
        "\n",
        "# The trivial predictor returns the average value.\n",
        "def trivial_predictor(X_test, y_train_avg):\n",
        "  return y_train_avg\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AvHIWuD_XIu",
        "outputId": "c0fa9feb-196c-4195-c628-16146cb0b7a0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average of y on the training label values is 5.878764675855028\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.6 test baseline model \n",
        "we now evaluate the trivial predictor on the training data and test data. We use mean squared error (MSE) to measure the performance of the predictor. "
      ],
      "metadata": {
        "id": "5vLZiJbr_vLA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement a function that can report the mean squared error of a predictor on the given data\n",
        "# Input: data and predictor\n",
        "# Output: mean squared error of the predictor on the given data\n",
        "\n",
        "def test_predictor(X, y, predictor: callable=None):\n",
        "    \n",
        "    y_predicted = np.apply_along_axis(predictor, 1, X) # apply the predictor to each row of X to get the predictions\n",
        "    mse = np.square(np.subtract(y, y_predicted)).mean() # compute mse\n",
        "\n",
        "    return mse\n",
        "\n",
        "# we use the lambda function here to pass the function trivial predictor to the function test_predictor.\n",
        "mse_trivial_predictor_train = test_predictor(X_train, y_train, lambda x: trivial_predictor(x, y_train_avg))\n",
        "mse_trivial_predictor_test = test_predictor(X_test, y_test, lambda x: trivial_predictor(x, y_train_avg))\n",
        "\n",
        "# Report the result\n",
        "print('Trivial Predictor')\n",
        "print('--------------------------------------------------------------------------------\\n')\n",
        "print('MSE (Training) = %.4f' % mse_trivial_predictor_train)\n",
        "print('MSE (Testing)  = %.4f' % mse_trivial_predictor_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fe9v1z_v_66S",
        "outputId": "33d968b3-ea91-4526-be1b-16ea602817df"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trivial Predictor\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "MSE (Training) = 0.7768\n",
            "MSE (Testing)  = 0.8139\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.7 standardize the datasets\n",
        "Before training the model, we need to standardize the data, i.e., transform the data so that every feature has mean 0 and variance 1. \n",
        "\n",
        "https://en.wikipedia.org/wiki/Standard_score\n",
        "\n",
        "We first standardize the training data. \n",
        "Then we apply the same transformation to the test data, i.e. standardize the test data using the means and the standard deviations of the training data. "
      ],
      "metadata": {
        "id": "usKvd3jUCBNJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# implement a function that can standardize the data and returns the mean and std of the data.\n",
        "# Input: training data\n",
        "# Output: standardize training data, standard deviations and means\n",
        "def standardize_data(X):\n",
        "    mean = np.mean(X, axis = 0)\n",
        "    sigma = np.std(X, axis = 0)\n",
        "    X_std = (X - mean)/sigma\n",
        "\n",
        "    return X_std, mean, sigma\n",
        "\n",
        "# Standardize the teaining data \n",
        "X_train_std, X_train_mean, X_train_sigma = standardize_data(X_train)\n",
        "print(\"X_train_std:\", X_train_std.shape)\n",
        "print(\"Mean:\", X_train_mean)\n",
        "print(\"Standard deviation:\", X_train_sigma)\n",
        "\n",
        "# Standardize the test data using the means and standrad deviations of the training data\n",
        "X_test_std = (X_test - X_train_mean)/X_train_sigma\n",
        "print(X_test_std.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6rdcY-fCVfW",
        "outputId": "62d7cc9d-6421-464e-fa0f-5aaf7ad6befa"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train_std: (3918, 11)\n",
            "Mean: [6.85427514e+00 2.78390761e-01 3.34892802e-01 6.42623788e+00\n",
            " 4.58213374e-02 3.53263144e+01 1.38513272e+02 9.94040729e-01\n",
            " 3.18647524e+00 4.89055641e-01 1.05115799e+01]\n",
            "Standard deviation: [8.39100902e-01 9.95630176e-02 1.24249975e-01 5.06377532e+00\n",
            " 2.16660282e-02 1.71004677e+01 4.23956179e+01 2.97972269e-03\n",
            " 1.49949475e-01 1.12992053e-01 1.22536544e+00]\n",
            "(980, 11)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Note : We have standardized X-values. Do we need to standardize the y-values? Why?  \n",
        "We don't need to standarize the y-value. The reason why we standardize X-value is it is input of linear regression model which includes interaction terms and polynomial terms. This means that if the independent variables (each features) have significant difference in scale, bias can be introduced in those terms and mislead the result. However, y-values are output value of our model, which has only one feature : quality of wine scored between 1 to 9. Accordingly, it makes more sense to keep the y-values as original values, otherwise standarization will rather change the meaning of score. For this reason standatization for y-value is unneccessary. \n",
        "  \n"
      ],
      "metadata": {
        "id": "YYQ9NAefDUfV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.8 Train the Linear Model using Least Squares Method\n",
        "\n",
        "Let us train a linear model on the training data and then check its MSE. \n",
        "We use the closed form solution of the least squares estimate to get the parameters of the linear model.  \n",
        "\n",
        " * Don't forget to add the bias term to the matrix X. \n",
        " * The linear model should perform better than the trivial predictor which returns simple mean of the labels!\n",
        "\n"
      ],
      "metadata": {
        "id": "RvPubPD_Bg_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# implement a function that adds a column of ones to the front of the input matrix\n",
        "def expand_with_ones(X):\n",
        "\n",
        "    n, m = X.shape\n",
        "    X_ones = np.ones((n,1))\n",
        "    X_out = np.hstack((X_ones,X))\n",
        "    \n",
        "    return X_out\n",
        "\n",
        "def least_squares_compute_parameters(X_input, y):\n",
        "    # 1. add the bias column to the data\n",
        "    X = expand_with_ones(X_input)\n",
        "\n",
        "    # 2. compute the weight based on the expanded X and y using the least-squares method\n",
        "    XT = np.transpose(X)\n",
        "    w = np.dot(np.dot(np.linalg.inv(np.dot(XT,X)),XT),y)\n",
        "\n",
        "    return w\n",
        "\n",
        "w = least_squares_compute_parameters(X_train_std, y_train) \n",
        "print(\"w:\", w.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrXcKZckDx9l",
        "outputId": "99d99519-c9f2-4870-c05e-01ae3fdb28bb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w: (12,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement the linear model predictor\n",
        "# Input: test data and parameters\n",
        "# Output: predicted values\n",
        "def linear_model_predictor(X, w):\n",
        "    return np.dot(X, w)\n"
      ],
      "metadata": {
        "id": "8VTXV-rhEbJu"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.9 Evaluate the linear model \n",
        "using test datset and compute its mse "
      ],
      "metadata": {
        "id": "OI_pUg2dEjLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use the function test_predictor to evaluate the linear model predictor\n",
        "mse_linear_model_predictor = test_predictor(expand_with_ones(X_test_std), y_test, lambda x: linear_model_predictor(x, w))\n",
        "print(\"Mean squared error is {}\".format(mse_linear_model_predictor))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "122K4ddJEqDm",
        "outputId": "95cf1ba1-8be6-49ba-94e0-ed03ef9c4eba"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean squared error is 0.5607292042283472\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.10 plot the Learning curve\n",
        "\n",
        "\n",
        "Let us check if the linear model is overfitting or underfitting. Since the dataset is somewhat large and there are only 11 features, the model shouldn't be overfitting.  \n",
        "\n",
        "To check it, we need to check the learning curves, i.e. how the performance of the model changes when it is trained with increasingly more data. \n",
        "We train the model on the increasingly more data ([20, 40, ..., 600] data records), and evaluate the model by computing the MSE of the model on both the training data and the test data. \n"
      ],
      "metadata": {
        "id": "9KWT8-Y0E2uC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input: training data and test data\n",
        "# Output: mse of the linear model predictor on both the training and test data\n",
        "def train_and_test(X_train, y_train, X_test, y_test):\n",
        "    \n",
        "    #1. standarize\n",
        "    X_train_std, X_train_mean, X_train_sigma = standardize_data(X_train)\n",
        "    X_test_std = (X_test - X_train_mean)/X_train_sigma\n",
        "\n",
        "    #2. calculate w \n",
        "    W = least_squares_compute_parameters(X_train_std, y_train)\n",
        "\n",
        "    #3. calculate mse \n",
        "    mse_train = test_predictor(expand_with_ones(X_train_std), y_train, lambda x: linear_model_predictor(x, W))\n",
        "    mse_test = test_predictor(expand_with_ones(X_test_std), y_test, lambda x: linear_model_predictor(x, W))\n",
        "\n",
        "    return mse_train, mse_test\n",
        "\n",
        "mse_train, mse_test = train_and_test(X_train, y_train, X_test, y_test)\n",
        "\n",
        "print('Linear Model')\n",
        "print('-----------------------\\n')\n",
        "print('MSE (Training) = %.4f' % mse_train)\n",
        "print('MSE (Testing)  = %.4f' % mse_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fclSIp6-TnsI",
        "outputId": "68379324-1751-4d84-f322-af24cfa879ad"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Model\n",
            "-----------------------\n",
            "\n",
            "MSE (Training) = 0.5640\n",
            "MSE (Testing)  = 0.5607\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mse_train_v = []\n",
        "mse_test_v = []\n",
        "\n",
        "TRAINING_SIZE_MAX = 601\n",
        "TRAINING_SIZE_MIN = 20\n",
        "\n",
        "# compute the MSE over data with sizes from TRAINING_SIZE_MIN to TRAINING_SIZE_MAX with increasing step 20\n",
        "for train_size in range(TRAINING_SIZE_MIN, TRAINING_SIZE_MAX, 20):\n",
        "  #  1. use the first train_size data records from the X_train and y_train as the training data\n",
        "  #  2. train and compute the MSE on both training and test data using the train_and_test function\n",
        "  #  3. add the computed MSE to the lists mse_train_v and mse_test_v\n",
        "\n",
        "    X_train_batch = X[:train_size]\n",
        "    y_train_batch = y[:train_size]\n",
        "    mse_train, mse_test = train_and_test(X_train_batch, y_train_batch, X_test, y_test)\n",
        "    \n",
        "    mse_train_v.append(mse_train)\n",
        "    mse_test_v.append(mse_test)\n",
        "\n",
        "# The below code generates the learning curves plot\n",
        "plt.figure(2)\n",
        "plt.title(\"Learning Curve\")\n",
        "plt.plot(np.arange(TRAINING_SIZE_MIN, TRAINING_SIZE_MAX, 20), mse_train_v, 'r--', label=\"Training Error\")\n",
        "plt.plot(np.arange(TRAINING_SIZE_MIN, TRAINING_SIZE_MAX, 20), mse_test_v, 'b-', label=\"Test Error\")\n",
        "plt.xlabel('Dataset Size')\n",
        "plt.ylabel('Mean Squared Error')\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "VUHXI6Y_T1qK",
        "outputId": "4acab974-d33f-4eda-b637-09eecb71102d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.11 Interpretation of the result \n",
        "Learning curve shows the trend of the training and test error as function of the training data size. In our result, both errors quickly start to plateau and meet at the same level after about 300 data is used. This means adding more data does not decrease both errors. However, in our case, both training and test error is pretty low enough, we can say our model fits and generalizes the dataset well. "
      ],
      "metadata": {
        "id": "H7V10aVCUTmG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task2. Improve linear models with \n",
        "    - regularization (Ridge and Lasso)\n",
        "    - polynomial basis expansion\n",
        "    - use validation data to choose the hyperparameters\n",
        "    - Use k-fold cross validation to choose the optimal hyperparameters"
      ],
      "metadata": {
        "id": "tYMdK-y07wpv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 import pakages\n"
      ],
      "metadata": {
        "id": "6TyJv27SU389"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import the preprocessing libs for standarization and basis expansion\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures \n",
        "\n",
        "# Ridge and Lasso linear model\n",
        "from sklearn.linear_model import Ridge, Lasso "
      ],
      "metadata": {
        "id": "UUsr_yMLU6dR"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 expand basis\n",
        "Let's implement the function for expanding the basis of the dataset using `PolynomialFeatures`"
      ],
      "metadata": {
        "id": "loNC8-YCU7PV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def expand_basis(X, degree):\n",
        "    # expand the basis of X for the input degree\n",
        "    poly = PolynomialFeatures(degree)\n",
        "    return poly.fit_transform(X)\n"
      ],
      "metadata": {
        "id": "N8ci3QUgVIXK"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Data preparation \n",
        "We need to expand and standardize the the data,\n",
        "and prepare the training, test and validation data.\n",
        "You should set the last 20% of the training data as the validation data.\n",
        "\n",
        "* use `StandardScaler` and `std_scaler` to standardize the data\n",
        "\n",
        "**NOTE**\n",
        "\n",
        "**Why standardisation is required before basis expansion?**\n",
        "\n",
        "Assume we have a dataset with two features x1 and x2, where x1 has a small scale while x2 has a large scale. When we perform basis expansion, we  get a new feature x1x2. Since x2 has a larger scale than x1, it is likely x2 will contribute more to the value of the new feature x1x2, which means a bias is introduce here. \n",
        "The correct way is to standardise the features before the basis expansion. In this case x1 and x2 have the same scale, so they contribute same to the new feature x1x2, i.e. no bias is introduced. \n",
        "\n",
        "\n",
        "**Why standardise the training data in step 5?**\n",
        "\n",
        "Ridge and Lasso regularisation require the data to have mean of 0 and standard deviation of 1. However, after the basis expansion and splitting in step 4, the training data might not have the desired distribution, so we need to perform the standardisation on the training data. \n",
        "\n",
        " \n",
        "**Why not standardise both training and validation data together?**\n",
        "\n",
        "When we use validation data to chose the hyperparameters, we treat the validation data like the test data -- we should not assume we can access these data. So we should standardise the training data and perform the same operation to the validation data. "
      ],
      "metadata": {
        "id": "1_0VPJATVjsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(X, y, degree):\n",
        "\n",
        "    # 1. split the data (X, y) into training data (X_train, y_train) and test data (X_test, y_test)\n",
        "    # 2. standardize the training data and do the same transformation to the test data\n",
        "    # 3. expand the basis of the training data and test data\n",
        "    # 4. split the expanded training data into training data (X_train_n, y_train_n) and validation data (X_train_v, y_train_v)\n",
        "    # 5. standardize the training data and do the same transformation to the validation data\n",
        "\n",
        "    #1. split train / test dataset\n",
        "    X_train, y_train, X_test, y_test = split_data(X, y, 0.8)\n",
        "\n",
        "    #2. standarize\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    #3. basis expansion\n",
        "    X_train = expand_basis(X_train, degree)\n",
        "    X_test = expand_basis(X_test, degree)\n",
        "\n",
        "    #4. split train / validation dataset\n",
        "    X_train_n, y_train_n, X_train_v, y_train_v = split_data(X_train, y_train, 0.8)\n",
        "    \n",
        "    #5. standarize \n",
        "    X_train_n= scaler.fit_transform(X_train_n)\n",
        "    X_train_v= scaler.transform(X_train_v)\n",
        "\n",
        "\n",
        "    return X_train, y_train, X_train_n, y_train_n, X_train_v, y_train_v, X_test, y_test\n",
        "\n",
        "X_train, y_train, X_train_n, y_train_n, X_train_v, y_train_v, X_test, y_test = prepare_data(X, y, 2) # here we expand the dataset with degree 2"
      ],
      "metadata": {
        "id": "ZnMniI49rmUF"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4 find hyperparameters (lambda) for Ridge and Lasso model\n",
        "\n",
        "We use the Ridge and Lasso models from scikit-learn: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html\n",
        "\n",
        "We train Ridge or Lasso models with different lambda values (from 10^-4 to 10^2) and check their performance on the validation data. The lambda value that results the best performance is then the optimal lambda."
      ],
      "metadata": {
        "id": "p97hq0RCsDPo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The function takes the training and validation data as inputs, and returns the lambda value that results the minimal mse\n",
        "# We use is_ridge to indicate which the model is considered (is_ridge = True indicates Ridge while is_ridge = False indicates Lasso)\n",
        "def choose_hyper_param(X_train_n, y_train_n, X_train_v, y_train_v, is_ridge: bool):\n",
        "    mse_list = []\n",
        "    lam_list = []\n",
        "\n",
        "    for pow_lam in range(-4, 3):\n",
        "        lam = 10 ** pow_lam\n",
        "\n",
        "        if is_ridge:\n",
        "          reg = Ridge(lam)\n",
        "        else:\n",
        "          reg = Lasso(lam)\n",
        "  \n",
        "        reg.fit(X_train_n, y_train_n)\n",
        "\n",
        "        y_hat = reg.predict(X_train_v)\n",
        "        mse = (np.square(y_train_v - y_hat)).mean()\n",
        "\n",
        "        mse_list.append(mse) \n",
        "        lam_list.append(lam)\n",
        "\n",
        "    # get the index of the lambda value that has the minimal use\n",
        "    lambda_idx_min = np.argmin(np.array(mse_list))\n",
        "\n",
        "    # plot of the lambda values and their mse\n",
        "    plt.figure()\n",
        "    plt.semilogx(lam_list, mse_list)\n",
        "\n",
        "    # return the optimal lambda value\n",
        "    return lam_list[lambda_idx_min]\n",
        "\n",
        "# call the function to choose the lambda for Ridge and Lasso\n",
        "lam_ridge = choose_hyper_param(X_train_n, y_train_n, X_train_v, y_train_v, True)\n",
        "lam_lasso = choose_hyper_param(X_train_n, y_train_n, X_train_v, y_train_v, False)\n",
        "\n",
        "print(\"Ridge lambda:\", lam_ridge)\n",
        "print(\"Lasso lambda:\", lam_lasso)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        },
        "id": "G9uYnmbasW4f",
        "outputId": "698be5af-d1ea-410b-bebc-402e25da816f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.680e+02, tolerance: 2.424e-01\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.041e-01, tolerance: 2.424e-01\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ridge lambda: 1\n",
            "Lasso lambda: 0.001\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.5 train models with optimal hyperparameters\n",
        "\n"
      ],
      "metadata": {
        "id": "nEXmv9KAtTF2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params = [(\"Ridge\",Ridge,lam_ridge), (\"Lasso\",Lasso,lam_lasso)]\n",
        "res = {}\n",
        "\n",
        "for param in params:\n",
        "  res[param[0]] = []\n",
        "\n",
        "  reg = param[1](param[2])\n",
        "  reg.fit(X_train, y_train)\n",
        "  \n",
        "  y_train_hat = reg.predict(X_train)\n",
        "  mse_train = (np.square(y_train - y_train_hat)).mean()\n",
        "  res[param[0]].append(mse_train)\n",
        "  \n",
        "  y_test_hat = reg.predict(X_test)\n",
        "  mse_test = (np.square(y_test - y_test_hat)).mean()\n",
        "  res[param[0]].append(mse_test)\n",
        "\n",
        "mse_ridge_train, mse_ridge_test, mse_lasso_train, mse_lasso_test = res[\"Ridge\"][0], res[\"Ridge\"][1], res[\"Lasso\"][0], res[\"Lasso\"][1]\n",
        "\n",
        "\n",
        "# Report the result\n",
        "print('For Ridge Regression with using degree %d polynomial expansion and lambda = %.4f' % (2, lam_ridge))\n",
        "print('--------------------------------------------------------------------------------\\n')\n",
        "print('MSE (Training) = %.4f' % mse_ridge_train)\n",
        "print('MSE (Testing)  = %.4f' % mse_ridge_test)\n",
        "\n",
        "print('\\n\\nFor Lasso with using degree %d polynomial expansion and lambda = %.4f' % (2, lam_lasso))\n",
        "print('---------------------------------------------------------------------\\n')\n",
        "print('MSE (Training) = %.4f' % mse_lasso_train)\n",
        "print('MSE (Testing)  = %.4f' % mse_lasso_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hO18Alu4tcUc",
        "outputId": "c138c9fa-6eb3-4edf-d4b0-90a08e61f320"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For Ridge Regression with using degree 2 polynomial expansion and lambda = 1.0000\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "MSE (Training) = 0.4952\n",
            "MSE (Testing)  = 0.5125\n",
            "\n",
            "\n",
            "For Lasso with using degree 2 polynomial expansion and lambda = 0.0010\n",
            "---------------------------------------------------------------------\n",
            "\n",
            "MSE (Training) = 0.4962\n",
            "MSE (Testing)  = 0.5098\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.057e-01, tolerance: 3.043e-01\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.6  basis expansions with higher degrees \n",
        "try basis expansions with higher degrees (up to degree 4) and find the degree that results the best performance. \n",
        "\n",
        "Instead of always using the same validation set, you should use k-fold cross validation to find the optimal hyperparameters. (use `KFold`)\n",
        "\n",
        "report the optimal hyperparameters (the basis expansion degree and the lambdas) and the MSE of the Ridge and Lasso when you apply the optimal hyperparameters. \n",
        "\n"
      ],
      "metadata": {
        "id": "KjfdhLYwtrIX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold"
      ],
      "metadata": {
        "id": "6xvW4fuQuLrd"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. split dataset to train and test set with the ratio of 80:20\n",
        "X_train, y_train, X_test, y_test = split_data(X, y, 0.8)\n",
        "\n",
        "# 2. Standarize input data \n",
        "X_train_std, X_train_mean, X_train_sigma = standardize_data(X_train)\n",
        "X_test_std = (X_test - X_train_mean)/X_train_sigma\n",
        "\n",
        "# 3. Find a optimal model with different hyperparameters (polynomial bais expansion degree 1 to 4 and regularizer lambda 10^-4 to 10^3)\n",
        "degree = [1,2,3,4]\n",
        "lams = [10**x for x in range(-4,4)]\n",
        "res = { \"ridge\" : {\"mse_list\": [],\"hyp_list\": []}, #degree, lamda\n",
        "        \"lasso\" : {\"mse_list\": [],\"hyp_list\": []}}\n",
        "params = [(\"ridge\", Ridge),(\"lasso\", Lasso)]\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "kf = KFold(n_splits=5)\n",
        "\n",
        "for i in degree:\n",
        "  # basis expansion based on different degree 1 to 4\n",
        "  X_train = expand_basis(X_train_std, i)\n",
        "  X_test = expand_basis(X_test_std, i)\n",
        "\n",
        "  mse_list_r = [] # degree i with min lambda of ridge model\n",
        "  mse_list_l = [] # degree i with min lambda of lasso model\n",
        "\n",
        "  mse = { \"ridge\": [],\n",
        "          \"lasso\": []}\n",
        "  lam_list = []\n",
        "\n",
        "  # test for each model with lambda 10^-4 to 10^3\n",
        "  for lam in lams:\n",
        "    kf_mse = { \"ridge\": [],\n",
        "                \"lasso\": []\n",
        "               }\n",
        "\n",
        "    # validate with kf = 5 \n",
        "    for n_i, v_i in kf.split(X_train):\n",
        "      X_train_n, X_train_v = X_train[n_i], X_train[v_i]\n",
        "      y_train_n, y_train_v = y_train[n_i], y_train[v_i]\n",
        "\n",
        "      # standarise train and validation set \n",
        "      X_train_n = scaler.fit_transform(X_train_n)\n",
        "      X_train_v = scaler.transform(X_train_v)\n",
        "\n",
        "      # train with each regularizer and store mse \n",
        "      for param in params:\n",
        "        reg = param[1](lam)\n",
        "        reg.fit(X_train_n, y_train_n)\n",
        "        y_hat = reg.predict(X_train_v)\n",
        "        mse = (np.square(y_train_v - y_hat)).mean()\n",
        "        kf_mse[param[0]].append(mse)\n",
        "\n",
        "    #calculate mean of kf_mse and store\n",
        "    mse_list_r.append(np.mean(kf_mse[\"ridge\"]))\n",
        "    mse_list_l.append(np.mean(kf_mse[\"lasso\"])) \n",
        "    lam_list.append(lam)\n",
        "\n",
        "  res[\"ridge\"][\"mse_list\"].append(min(mse_list_r))\n",
        "  res[\"lasso\"][\"mse_list\"].append(min(mse_list_l))\n",
        "\n",
        "  lambda_idx_min_r = np.argmin(np.array(mse_list_r))\n",
        "  res[\"ridge\"][\"hyp_list\"].append((i, lam_list[lambda_idx_min_r]))\n",
        "  lambda_idx_min_l = np.argmin(np.array(mse_list_l))\n",
        "  res[\"lasso\"][\"hyp_list\"].append((i, lam_list[lambda_idx_min_l]))\n",
        "  \n"
      ],
      "metadata": {
        "id": "I7s4a6F0uMIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. find optimal hyps\n",
        "\n",
        "min_mse_r = min(res[\"ridge\"][\"mse_list\"])\n",
        "hyp_idx_min_r = np.argmin(np.array(res[\"ridge\"][\"mse_list\"]))\n",
        "optimal_hyp_r = res[\"ridge\"][\"hyp_list\"][hyp_idx_min_r]\n",
        "\n",
        "min_mse_l = min(res[\"lasso\"][\"mse_list\"])\n",
        "hyp_idx_min_l = np.argmin(np.array(res[\"lasso\"][\"mse_list\"]))\n",
        "optimal_hyp_l = res[\"lasso\"][\"hyp_list\"][hyp_idx_min_l]\n",
        "\n",
        "# 5. report optimal hyps\n",
        "print(\"The optimal hyperparameters for Ridge model: degree %d and lambda %4f with validation mse %4f\" % (optimal_hyp_r[0], optimal_hyp_r[1], min_mse_r) )\n",
        "print(\"The optimal hyperparameters for Lasso model: degree %d and lambda %4f with validation mse %4f\" % (optimal_hyp_l[0], optimal_hyp_l[1], min_mse_l) )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSkLEClauQtm",
        "outputId": "29ddd595-a37e-417e-dea6-22f7f1178f7e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The optimal hyperparameters for Ridge model: degree 2 and lambda 10.000000 with validation mse 0.538901\n",
            "The optimal hyperparameters for Lasso model: degree 2 and lambda 0.001000 with validation mse 0.537769\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Report tarin error and test error of model with optimal hyperpameter \n",
        "\n",
        "def mse_optimal(model, lam, X_train, y_train, X_test, y_test):\n",
        "  reg = model(lam)  \n",
        "  reg.fit(X_train, y_train)\n",
        "  y_train_hat = reg.predict(X_train)\n",
        "  mse_train = (np.square(y_train - y_train_hat)).mean()\n",
        "\n",
        "  y_test_hat = reg.predict(X_test)\n",
        "  mse_test = (np.square(y_test - y_test_hat)).mean()\n",
        "\n",
        "  return mse_train, mse_test"
      ],
      "metadata": {
        "id": "Wj2WxLoiuVG0"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Split train / test dataset\n",
        "X_train, y_train, X_test, y_test = split_data(X, y, 0.8)\n",
        "\n",
        "#standarize\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "#basis expansion\n",
        "X_train = expand_basis(X_train, 2)\n",
        "X_test = expand_basis(X_test, 2)\n",
        "\n",
        "# calculate tarin error and test error\n",
        "mse_ridge_train, mse_ridge_test = mse_optimal(Ridge, optimal_hyp_r[1], X_train, y_train, X_test, y_test)\n",
        "mse_lasso_train, mse_lasso_test = mse_optimal(Lasso,optimal_hyp_l[1], X_train, y_train, X_test, y_test)\n",
        "\n",
        "print('For Ridge Regression with using degree %d polynomial expansion and lambda = %.3f' % (optimal_hyp_r[0], optimal_hyp_r[1]))\n",
        "print('--------------------------------------------------------------------------------\\n')\n",
        "print('MSE (Training) = %.4f' % mse_ridge_train)\n",
        "print('MSE (Testing)  = %.4f' % mse_ridge_test)\n",
        "\n",
        "print('\\n\\nFor Lasso with using degree %d polynomial expansion and lambda = %.3f' % (optimal_hyp_l[0], optimal_hyp_l[1]))\n",
        "print('---------------------------------------------------------------------\\n')\n",
        "print('MSE (Training) = %.4f' % mse_lasso_train)\n",
        "print('MSE (Testing)  = %.4f' % mse_lasso_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbUQjcxBudo7",
        "outputId": "cdc67e67-ffcb-45e5-cf67-e56b5e49a3a4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For Ridge Regression with using degree 2 polynomial expansion and lambda = 10.000\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "MSE (Training) = 0.4955\n",
            "MSE (Testing)  = 0.5117\n",
            "\n",
            "\n",
            "For Lasso with using degree 2 polynomial expansion and lambda = 0.001\n",
            "---------------------------------------------------------------------\n",
            "\n",
            "MSE (Training) = 0.4962\n",
            "MSE (Testing)  = 0.5098\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.057e-01, tolerance: 3.043e-01\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation**  \n",
        "\n",
        "It is interesting that we got different lambda value for Ridge model, even though with the same polynomial degree that we tried above. We think this is because Ridge model is somewwhat sensitive to different train/validation set. it seems around 4000 data set is not enough to get stable results.  "
      ],
      "metadata": {
        "id": "RBJhdBEbuumX"
      }
    }
  ]
}